<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Building a Cat Image Classifier using Neural Networks from Scratch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-roman.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-mono.css">
</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#library-imports-and-environment-setup" id="toc-library-imports-and-environment-setup" class="nav-link" data-scroll-target="#library-imports-and-environment-setup">Library Imports and Environment Setup</a></li>
  </ul></li>
  <li><a href="#data-loading-and-exploration" id="toc-data-loading-and-exploration" class="nav-link" data-scroll-target="#data-loading-and-exploration">Data Loading and Exploration</a>
  <ul class="collapse">
  <li><a href="#dataset-loading-and-preprocessing" id="toc-dataset-loading-and-preprocessing" class="nav-link" data-scroll-target="#dataset-loading-and-preprocessing">Dataset Loading and Preprocessing</a></li>
  <li><a href="#visualizing-dataset-samples-and-labels" id="toc-visualizing-dataset-samples-and-labels" class="nav-link" data-scroll-target="#visualizing-dataset-samples-and-labels">Visualizing Dataset Samples and Labels</a></li>
  <li><a href="#dataset-overview-and-structure" id="toc-dataset-overview-and-structure" class="nav-link" data-scroll-target="#dataset-overview-and-structure">Dataset Overview and Structure</a></li>
  </ul></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data Preprocessing</a>
  <ul class="collapse">
  <li><a href="#data-reshaping-and-normalization" id="toc-data-reshaping-and-normalization" class="nav-link" data-scroll-target="#data-reshaping-and-normalization">Data Reshaping and Normalization</a></li>
  </ul></li>
  <li><a href="#neural-network-architecture" id="toc-neural-network-architecture" class="nav-link" data-scroll-target="#neural-network-architecture">Neural Network Architecture</a>
  <ul class="collapse">
  <li><a href="#neural-network-architecture-definition" id="toc-neural-network-architecture-definition" class="nav-link" data-scroll-target="#neural-network-architecture-definition">Neural Network Architecture Definition</a></li>
  <li><a href="#deep-neural-network-parameter-initialization" id="toc-deep-neural-network-parameter-initialization" class="nav-link" data-scroll-target="#deep-neural-network-parameter-initialization">Deep Neural Network Parameter Initialization</a></li>
  <li><a href="#activation-functions-implementation-sigmoid-and-relu" id="toc-activation-functions-implementation-sigmoid-and-relu" class="nav-link" data-scroll-target="#activation-functions-implementation-sigmoid-and-relu">Activation Functions Implementation (Sigmoid and ReLU)</a></li>
  </ul></li>
  <li><a href="#forward-propagation" id="toc-forward-propagation" class="nav-link" data-scroll-target="#forward-propagation">Forward Propagation</a>
  <ul class="collapse">
  <li><a href="#forward-propagation-implementation-for-neural-network-layers" id="toc-forward-propagation-implementation-for-neural-network-layers" class="nav-link" data-scroll-target="#forward-propagation-implementation-for-neural-network-layers">Forward Propagation Implementation for Neural Network Layers</a></li>
  <li><a href="#forward-propagation-in-l-layer-neural-networks" id="toc-forward-propagation-in-l-layer-neural-networks" class="nav-link" data-scroll-target="#forward-propagation-in-l-layer-neural-networks">Forward Propagation in L-layer Neural Networks</a></li>
  </ul></li>
  <li><a href="#cost-computation" id="toc-cost-computation" class="nav-link" data-scroll-target="#cost-computation">Cost Computation</a>
  <ul class="collapse">
  <li><a href="#cross-entropy-cost-function-implementation" id="toc-cross-entropy-cost-function-implementation" class="nav-link" data-scroll-target="#cross-entropy-cost-function-implementation">Cross-Entropy Cost Function Implementation</a></li>
  </ul></li>
  <li><a href="#backward-propagation" id="toc-backward-propagation" class="nav-link" data-scroll-target="#backward-propagation">Backward Propagation</a>
  <ul class="collapse">
  <li><a href="#backward-propagation-derivation-and-implementation-for-neural-network-layers" id="toc-backward-propagation-derivation-and-implementation-for-neural-network-layers" class="nav-link" data-scroll-target="#backward-propagation-derivation-and-implementation-for-neural-network-layers">Backward Propagation Derivation and Implementation for Neural Network Layers</a></li>
  <li><a href="#gradient-descent-and-parameter-updates" id="toc-gradient-descent-and-parameter-updates" class="nav-link" data-scroll-target="#gradient-descent-and-parameter-updates">Gradient Descent and Parameter Updates</a></li>
  </ul></li>
  <li><a href="#model-training" id="toc-model-training" class="nav-link" data-scroll-target="#model-training">Model Training</a>
  <ul class="collapse">
  <li><a href="#training-of-deep-neural-network-models" id="toc-training-of-deep-neural-network-models" class="nav-link" data-scroll-target="#training-of-deep-neural-network-models">Training of Deep Neural Network Models</a></li>
  <li><a href="#gradient-descent-and-parameter-updates-1" id="toc-gradient-descent-and-parameter-updates-1" class="nav-link" data-scroll-target="#gradient-descent-and-parameter-updates-1">Gradient Descent and Parameter Updates</a></li>
  </ul></li>
  <li><a href="#model-evaluation" id="toc-model-evaluation" class="nav-link" data-scroll-target="#model-evaluation">Model Evaluation</a>
  <ul class="collapse">
  <li><a href="#model-performance-evaluation-and-accuracy-assessment" id="toc-model-performance-evaluation-and-accuracy-assessment" class="nav-link" data-scroll-target="#model-performance-evaluation-and-accuracy-assessment">Model Performance Evaluation and Accuracy Assessment</a></li>
  </ul></li>
  <li><a href="#results-visualization" id="toc-results-visualization" class="nav-link" data-scroll-target="#results-visualization">Results Visualization</a>
  <ul class="collapse">
  <li><a href="#cost-evolution-during-training-iterations" id="toc-cost-evolution-during-training-iterations" class="nav-link" data-scroll-target="#cost-evolution-during-training-iterations">Cost Evolution During Training Iterations</a></li>
  <li><a href="#visualizing-misclassified-images" id="toc-visualizing-misclassified-images" class="nav-link" data-scroll-target="#visualizing-misclassified-images">Visualizing Misclassified Images</a></li>
  </ul></li>
  <li><a href="#conclusion-assessing-the-cat-image-classification-neural-network" id="toc-conclusion-assessing-the-cat-image-classification-neural-network" class="nav-link" data-scroll-target="#conclusion-assessing-the-cat-image-classification-neural-network">Conclusion: Assessing the Cat Image Classification Neural Network</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Building a Cat Image Classifier using Neural Networks from Scratch</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p><strong>Repo link</strong> : <a href="https://github.com/SherryS997/cat-image-classification-neural-network-numpy-scipy">https://github.com/SherryS997/cat-image-classification-neural-network-numpy-scipy</a></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The following Jupyter Notebook encapsulates the step-by-step creation of a cat image classifier using Neural Networks, entirely coded from scratch using Python libraries like NumPy, SciPy, and Matplotlib. This project aims to distinguish cat images from non-cat images using machine learning techniques implemented through a neural network architecture.</p>
<p>The notebook begins by loading the necessary libraries and datasets containing both training and test images of cats and non-cats. It further explores the dataset’s structure, dimensions, and the process of standardizing image data to be used in the neural network.</p>
<p>The construction of the neural network includes defining activation functions (sigmoid and ReLU), initializing parameters, implementing forward and backward propagation, and updating parameters via gradient descent. The model’s performance is assessed using various functions to compute the cost, make predictions, and visualize mislabeled images.</p>
<p>Additionally, the notebook contains a function to predict images outside the dataset, enabling users to test the model’s classification capabilities on their images.</p>
<section id="library-imports-and-environment-setup" class="level3">
<h3 class="anchored" data-anchor-id="library-imports-and-environment-setup">Library Imports and Environment Setup</h3>
<p>This section initializes the notebook by importing essential libraries and configuring the environment. It includes importing standard libraries such as NumPy, SciPy, Matplotlib, and PIL (Python Imaging Library). Additionally, the section sets up the notebook environment by configuring parameters for Matplotlib plots, ensuring consistent visualization settings throughout the notebook. This step is crucial as it prepares the groundwork for subsequent data handling, model building, and result visualization within the notebook.</p>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> h5py</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> ndimage</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> (<span class="fl">5.0</span>, <span class="fl">4.0</span>) <span class="co"># set default size of plots</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'image.interpolation'</span>] <span class="op">=</span> <span class="st">'nearest'</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'image.cmap'</span>] <span class="op">=</span> <span class="st">'gray'</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="data-loading-and-exploration" class="level2">
<h2 class="anchored" data-anchor-id="data-loading-and-exploration">Data Loading and Exploration</h2>
<section id="dataset-loading-and-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="dataset-loading-and-preprocessing">Dataset Loading and Preprocessing</h3>
<p>This section of the code focuses on loading the cat image dataset, which includes both training and test sets, from .h5 files. It reads the data and relevant labels, separating them into training features (<code>train_x_orig</code>) and labels (<code>train_y</code>), and test features (<code>test_x_orig</code>) and labels (<code>test_y</code>). Additionally, it retrieves the classes/categories from the dataset.</p>
<p>The data is loaded using the <code>h5py</code> library and converted into NumPy arrays for further processing. Reshaping and organizing the label data ensure compatibility with subsequent operations. This step is crucial as it lays the foundation for subsequent data preprocessing and model development stages.</p>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> h5py.File(<span class="st">'datasets/train_catvnoncat.h5'</span>, <span class="st">"r"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>train_x_orig <span class="op">=</span> np.array(train_dataset[<span class="st">"train_set_x"</span>][:]) <span class="co"># your train set features</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>train_y <span class="op">=</span> np.array(train_dataset[<span class="st">"train_set_y"</span>][:]) <span class="co"># your train set labels</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> h5py.File(<span class="st">'datasets/test_catvnoncat.h5'</span>, <span class="st">"r"</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>test_x_orig <span class="op">=</span> np.array(test_dataset[<span class="st">"test_set_x"</span>][:]) <span class="co"># your test set features</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>test_y <span class="op">=</span> np.array(test_dataset[<span class="st">"test_set_y"</span>][:]) <span class="co"># your test set labels</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>classes <span class="op">=</span> np.array(test_dataset[<span class="st">"list_classes"</span>][:]) <span class="co"># the list of classes</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>train_y <span class="op">=</span> train_y.reshape((<span class="dv">1</span>, train_y.shape[<span class="dv">0</span>]))</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>test_y <span class="op">=</span> test_y.reshape((<span class="dv">1</span>, test_y.shape[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="visualizing-dataset-samples-and-labels" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-dataset-samples-and-labels">Visualizing Dataset Samples and Labels</h3>
<p>This section presents a visualization of the dataset samples along with their respective labels. Using matplotlib, it displays a specific image from the training set identified by its index. Additionally, the label associated with the image is shown, providing clarity about the class it represents. In this instance, the output displays an example image labeled as a non-cat (y = 0), portraying a picture that corresponds to a hummingbird.</p>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of a picture</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>plt.imshow(train_x_orig[index])</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"y = "</span> <span class="op">+</span> <span class="bu">str</span>(train_y[<span class="dv">0</span>,index]) <span class="op">+</span> <span class="st">". It's a "</span> <span class="op">+</span> classes[train_y[<span class="dv">0</span>,index]].decode(<span class="st">"utf-8"</span>) <span class="op">+</span>  <span class="st">" picture."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>y = 0. It's a non-cat picture.</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Cat_classifier_files/figure-html/cell-4-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="dataset-overview-and-structure" class="level3">
<h3 class="anchored" data-anchor-id="dataset-overview-and-structure">Dataset Overview and Structure</h3>
<p>This section provides an insight into the dataset statistics, presenting essential information such as the number of training and testing examples, along with details regarding the dimensions and size of each image. It displays the number of training and testing examples, specifying the dimensions and shape of the image arrays. This exploration helps understand the dataset’s structure and prepares for preprocessing and model development.</p>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Explore your dataset </span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>m_train <span class="op">=</span> train_x_orig.shape[<span class="dv">0</span>]</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>num_px <span class="op">=</span> train_x_orig.shape[<span class="dv">1</span>]</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>m_test <span class="op">=</span> test_x_orig.shape[<span class="dv">0</span>]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"Number of training examples: "</span> <span class="op">+</span> <span class="bu">str</span>(m_train))</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"Number of testing examples: "</span> <span class="op">+</span> <span class="bu">str</span>(m_test))</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"Each image is of size: ("</span> <span class="op">+</span> <span class="bu">str</span>(num_px) <span class="op">+</span> <span class="st">", "</span> <span class="op">+</span> <span class="bu">str</span>(num_px) <span class="op">+</span> <span class="st">", 3)"</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"train_x_orig shape: "</span> <span class="op">+</span> <span class="bu">str</span>(train_x_orig.shape))</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"train_y shape: "</span> <span class="op">+</span> <span class="bu">str</span>(train_y.shape))</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"test_x_orig shape: "</span> <span class="op">+</span> <span class="bu">str</span>(test_x_orig.shape))</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"test_y shape: "</span> <span class="op">+</span> <span class="bu">str</span>(test_y.shape))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of training examples: 209
Number of testing examples: 50
Each image is of size: (64, 64, 3)
train_x_orig shape: (209, 64, 64, 3)
train_y shape: (1, 209)
test_x_orig shape: (50, 64, 64, 3)
test_y shape: (1, 50)</code></pre>
</div>
</div>
<p>The output from the code snippet provides crucial insights into the dataset structure and composition:</p>
<ul>
<li><p><strong>Number of training examples</strong>: The dataset comprises 209 training examples, which serves as the data used for training the classification model.</p></li>
<li><p><strong>Number of testing examples</strong>: There are 50 testing examples, utilized to evaluate the trained model’s performance on unseen data.</p></li>
<li><p><strong>Image size</strong>: Each image in the dataset has a dimension of (64, 64, 3), indicating that the images are 64 pixels in height and width with three color channels (RGB).</p></li>
<li><p><strong>Train and test set shapes</strong>: The training set, denoted by <code>train_x_orig</code>, consists of 209 images, each with a size of (64, 64, 3). The associated training labels (<code>train_y</code>) have a shape of (1, 209). Similarly, the test set (<code>test_x_orig</code>) comprises 50 images with the same dimensions as the training images, and the corresponding test labels (<code>test_y</code>) possess a shape of (1, 50).</p></li>
</ul>
<p>Understanding these dataset statistics is vital for various tasks, such as data preprocessing, designing the neural network architecture, and assessing the model’s performance during training and testing phases.</p>
</section>
</section>
<section id="data-preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="data-preprocessing">Data Preprocessing</h2>
<section id="data-reshaping-and-normalization" class="level3">
<h3 class="anchored" data-anchor-id="data-reshaping-and-normalization">Data Reshaping and Normalization</h3>
<p>This section involves the preprocessing steps applied to the dataset before feeding it into the neural network model. It includes two key operations:</p>
<ol type="1">
<li><p><strong>Reshaping</strong>: The code snippet reshapes the training and test examples. Using the <code>reshape()</code> function, the images are flattened to a 1D array while preserving the number of samples. This operation is crucial as it transforms the multi-dimensional image data into a format suitable for further processing.</p></li>
<li><p><strong>Normalization</strong>: Normalization is performed to standardize the pixel values of the images. The pixel values, initially ranging from 0 to 255, are scaled to be between 0 and 1 by dividing each pixel value by 255. Normalizing the data helps in achieving uniformity and stability during training, aiding the neural network to learn effectively without being skewed by varying pixel ranges. The standardized data is denoted as <code>train_x</code> and <code>test_x</code>.</p></li>
</ol>
<p>These preprocessing steps are essential for preparing the data before training the neural network, ensuring that the model learns effectively and efficiently from the input images.</p>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshape the training and test examples </span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>train_x_flatten <span class="op">=</span> train_x_orig.reshape(train_x_orig.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>).T   <span class="co"># The "-1" makes reshape flatten the remaining dimensions</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>test_x_flatten <span class="op">=</span> test_x_orig.reshape(test_x_orig.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>).T</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize data to have feature values between 0 and 1.</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>train_x <span class="op">=</span> train_x_flatten<span class="op">/</span><span class="fl">255.</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>test_x <span class="op">=</span> test_x_flatten<span class="op">/</span><span class="fl">255.</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"train_x's shape: "</span> <span class="op">+</span> <span class="bu">str</span>(train_x.shape))</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"test_x's shape: "</span> <span class="op">+</span> <span class="bu">str</span>(test_x.shape))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train_x's shape: (12288, 209)
test_x's shape: (12288, 50)</code></pre>
</div>
</div>
</section>
</section>
<section id="neural-network-architecture" class="level2">
<h2 class="anchored" data-anchor-id="neural-network-architecture">Neural Network Architecture</h2>
<section id="neural-network-architecture-definition" class="level3">
<h3 class="anchored" data-anchor-id="neural-network-architecture-definition">Neural Network Architecture Definition</h3>
<p>Description: This section defines the architecture of the neural network based on specific constants and parameters. It establishes the structure of the model by setting the dimensions of each layer, including the input layer, hidden layers, and output layer. The constants <code>n_x</code>, <code>n_h</code>, and <code>n_y</code> represent the sizes of the input features, hidden layers, and output respectively. Additionally, the <code>layers_dims</code> list specifies the dimensions of each layer in a 4-layer neural network, providing insight into the overall structure of the model. The <code>learning_rate</code> parameter, crucial for optimization, is also initialized here.</p>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">### CONSTANTS DEFINING THE MODEL ####</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>n_x <span class="op">=</span> num_px <span class="op">*</span> num_px <span class="op">*</span> <span class="dv">3</span> </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>n_h <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>n_y <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>layers_dims <span class="op">=</span> [<span class="dv">12288</span>, <span class="dv">20</span>, <span class="dv">7</span>, <span class="dv">5</span>, <span class="dv">1</span>] <span class="co">#  4-layer model</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.0075</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="deep-neural-network-parameter-initialization" class="level3">
<h3 class="anchored" data-anchor-id="deep-neural-network-parameter-initialization">Deep Neural Network Parameter Initialization</h3>
<p>The function “initialize_parameters_deep” sets up the initial values for the weights (<code>W</code>) and biases (<code>b</code>) of a deep neural network with multiple layers. It takes in an array (<code>layer_dims</code>) containing the dimensions of each layer in the network.</p>
<p>This function initializes the parameters for each layer (<code>W1</code>, <code>b1</code>, …, <code>WL</code>, <code>bL</code>) as a Python dictionary (<code>parameters</code>). For each layer <code>l</code>, it generates random weights (<code>Wl</code>) of shape <code>(layer_dims[l], layer_dims[l-1])</code> using a Gaussian distribution and scales them by <code>np.sqrt(layer_dims[l-1])</code>. The biases (<code>bl</code>) are initialized as zero vectors of shape <code>(layer_dims[l], 1)</code>.</p>
<p>The purpose of this initialization is to provide suitable starting values for the weights and biases, aiding in the convergence of the neural network during training. The appropriate initialization helps prevent issues like vanishing/exploding gradients and contributes to better learning in the subsequent training phases of the network.</p>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> initialize_parameters_deep(layer_dims):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    np.random.seed(<span class="dv">1</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    parameters <span class="op">=</span> {}</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> <span class="bu">len</span>(layer_dims)            <span class="co"># number of layers in the network</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, L):</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        parameters[<span class="st">'W'</span> <span class="op">+</span> <span class="bu">str</span>(l)] <span class="op">=</span> np.random.randn(layer_dims[l], layer_dims[l<span class="op">-</span><span class="dv">1</span>]) <span class="op">/</span> np.sqrt(layer_dims[l<span class="op">-</span><span class="dv">1</span>]) <span class="co">#*0.01</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        parameters[<span class="st">'b'</span> <span class="op">+</span> <span class="bu">str</span>(l)] <span class="op">=</span> np.zeros((layer_dims[l], <span class="dv">1</span>))</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span>(parameters[<span class="st">'W'</span> <span class="op">+</span> <span class="bu">str</span>(l)].shape <span class="op">==</span> (layer_dims[l], layer_dims[l<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span>(parameters[<span class="st">'b'</span> <span class="op">+</span> <span class="bu">str</span>(l)].shape <span class="op">==</span> (layer_dims[l], <span class="dv">1</span>))</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> parameters</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="activation-functions-implementation-sigmoid-and-relu" class="level3">
<h3 class="anchored" data-anchor-id="activation-functions-implementation-sigmoid-and-relu">Activation Functions Implementation (Sigmoid and ReLU)</h3>
<p>This section in the code comprises the implementation of two fundamental activation functions used in neural networks: Sigmoid and Rectified Linear Unit (ReLU).</p>
<ol type="1">
<li><strong>Sigmoid Activation Function</strong>:
<ul>
<li><strong>Definition</strong>: The sigmoid activation function is implemented to squash the input values between 0 and 1, facilitating non-linear transformations in neural networks.</li>
<li><strong>Implementation</strong>: The sigmoid function takes in any numpy array <code>Z</code> and computes the output <code>A</code> using the formula <code>A = 1 / (1 + np.exp(-Z))</code>. It returns the computed <code>A</code> and also caches the input <code>Z</code> for efficient backpropagation during the training process.</li>
<li><strong>Backward Propagation</strong>: The <code>sigmoid_backward</code> function computes the gradient of the cost with respect to <code>Z</code> during backpropagation. It utilizes the cached <code>Z</code> and the incoming gradient <code>dA</code> to compute <code>dZ</code>, which is essential for updating weights in the network.</li>
</ul></li>
<li><strong>ReLU (Rectified Linear Unit) Activation Function</strong>:
<ul>
<li><strong>Definition</strong>: ReLU is a widely used activation function that introduces non-linearity by setting all negative values to zero and leaving positive values unchanged.</li>
<li><strong>Implementation</strong>: The <code>relu</code> function takes the output <code>Z</code> from a linear layer and computes the post-activation output <code>A</code> using <code>A = np.maximum(0, Z)</code>. It also caches the input <code>Z</code> for efficient backward pass computation.</li>
<li><strong>Backward Propagation</strong>: The <code>relu_backward</code> function calculates the gradient of the cost with respect to <code>Z</code> for a single ReLU unit. It utilizes the cached <code>Z</code> and incoming gradient <code>dA</code> to compute <code>dZ</code>. It ensures that for <code>Z &lt;= 0</code>, the gradient <code>dZ</code> is set to zero to handle the non-differentiability at <code>Z = 0</code>.</li>
</ul></li>
</ol>
<p>These activation functions, Sigmoid and ReLU, play pivotal roles in introducing non-linearities within neural networks, aiding in learning complex patterns and enhancing the network’s representational capacity during training.</p>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(Z):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Implements the sigmoid activation in numpy</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Z -- numpy array of any shape</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">    A -- output of sigmoid(z), same shape as Z</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">    cache -- returns Z as well, useful during backpropagation</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>Z))</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    cache <span class="op">=</span> Z</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A, cache</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(Z):</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co">    Implement the RELU function.</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="co">    Z -- Output of the linear layer, of any shape</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="co">    A -- Post-activation parameter, of the same shape as Z</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="co">    cache -- a python dictionary containing "A" ; stored for computing the backward pass efficiently</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.maximum(<span class="dv">0</span>,Z)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(A.shape <span class="op">==</span> Z.shape)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>    cache <span class="op">=</span> Z </span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A, cache</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu_backward(dA, cache):</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a><span class="co">    Implement the backward propagation for a single RELU unit.</span></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a><span class="co">    dA -- post-activation gradient, of any shape</span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a><span class="co">    cache -- 'Z' where we store for computing backward propagation efficiently</span></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a><span class="co">    dZ -- Gradient of the cost with respect to Z</span></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> cache</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>    dZ <span class="op">=</span> np.array(dA, copy<span class="op">=</span><span class="va">True</span>) <span class="co"># just converting dz to a correct object.</span></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># When z &lt;= 0, you should set dz to 0 as well. </span></span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>    dZ[Z <span class="op">&lt;=</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> (dZ.shape <span class="op">==</span> Z.shape)</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dZ</span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid_backward(dA, cache):</span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a><span class="co">    Implement the backward propagation for a single SIGMOID unit.</span></span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a><span class="co">    dA -- post-activation gradient, of any shape</span></span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a><span class="co">    cache -- 'Z' where we store for computing backward propagation efficiently</span></span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a><span class="co">    dZ -- Gradient of the cost with respect to Z</span></span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-72"><a href="#cb11-72" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> cache</span>
<span id="cb11-73"><a href="#cb11-73" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-74"><a href="#cb11-74" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>Z))</span>
<span id="cb11-75"><a href="#cb11-75" aria-hidden="true" tabindex="-1"></a>    dZ <span class="op">=</span> dA <span class="op">*</span> s <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>s)</span>
<span id="cb11-76"><a href="#cb11-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-77"><a href="#cb11-77" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> (dZ.shape <span class="op">==</span> Z.shape)</span>
<span id="cb11-78"><a href="#cb11-78" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-79"><a href="#cb11-79" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dZ</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="forward-propagation" class="level2">
<h2 class="anchored" data-anchor-id="forward-propagation">Forward Propagation</h2>
<section id="forward-propagation-implementation-for-neural-network-layers" class="level3">
<h3 class="anchored" data-anchor-id="forward-propagation-implementation-for-neural-network-layers">Forward Propagation Implementation for Neural Network Layers</h3>
<p>The code snippet presents the implementation of forward propagation for a neural network’s layers, encompassing both linear transformation and activation functions. The following functions are integral components of this forward propagation process:</p>
<ol type="1">
<li><strong><code>linear_forward</code> Function</strong>:
<ul>
<li>This function computes the linear part of a layer’s forward propagation.</li>
<li>It calculates the pre-activation parameter <code>Z</code> using the weights <code>W</code>, activations <code>A</code> from the previous layer, and bias <code>b</code>.</li>
<li>The resulting pre-activation parameter <code>Z</code> is fundamental for subsequent activation functions.</li>
<li>The calculated values are cached in a dictionary (<code>cache</code>) containing <code>A</code>, <code>W</code>, and <code>b</code> for efficient computation during the backward pass.</li>
</ul></li>
</ol>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_forward(A, W, b):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Implement the linear part of a layer's forward propagation.</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Z -- the input of the activation function, also called pre-activation parameter </span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="co">    cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> W.dot(A) <span class="op">+</span> b</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(Z.shape <span class="op">==</span> (W.shape[<span class="dv">0</span>], A.shape[<span class="dv">1</span>]))</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    cache <span class="op">=</span> (A, W, b)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Z, cache</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li><strong><code>linear_activation_forward</code> Function</strong>:
<ul>
<li>This function implements the complete forward propagation for the layer, combining the linear transformation and an activation function (<code>sigmoid</code> or <code>ReLU</code>).</li>
<li>Depending on the specified activation function, it computes the post-activation value <code>A</code> using the pre-activation <code>Z</code> obtained from the <code>linear_forward</code> step.</li>
<li>The resultant post-activation value <code>A</code> along with the linear and activation function caches (<code>linear_cache</code> and <code>activation_cache</code>) are stored in a dictionary (<code>cache</code>). These caches are crucial for efficient computation during the subsequent backward pass.</li>
</ul></li>
</ol>
<p>Both functions work in tandem to compute the forward pass through a layer, incorporating linear transformations and different activation functions based on the specified requirements (<code>sigmoid</code> or <code>ReLU</code>). The caches obtained during this process facilitate the backward propagation for updating the neural network’s parameters during training.</p>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_activation_forward(A_prev, W, b, activation):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co">    A -- the output of the activation function, also called the post-activation value </span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co">    cache -- a python dictionary containing "linear_cache" and "activation_cache";</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co">             stored for computing the backward pass efficiently</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> activation <span class="op">==</span> <span class="st">"sigmoid"</span>:</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        Z, linear_cache <span class="op">=</span> linear_forward(A_prev, W, b)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        A, activation_cache <span class="op">=</span> sigmoid(Z)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> activation <span class="op">==</span> <span class="st">"relu"</span>:</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        Z, linear_cache <span class="op">=</span> linear_forward(A_prev, W, b)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        A, activation_cache <span class="op">=</span> relu(Z)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> (A.shape <span class="op">==</span> (W.shape[<span class="dv">0</span>], A_prev.shape[<span class="dv">1</span>]))</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    cache <span class="op">=</span> (linear_cache, activation_cache)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A, cache</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="forward-propagation-in-l-layer-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="forward-propagation-in-l-layer-neural-networks">Forward Propagation in L-layer Neural Networks</h3>
<p>The function <code>L_model_forward</code> orchestrates the forward propagation process within an L-layer neural network. It executes a sequence of [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computations. Here’s how it works:</p>
<ol type="1">
<li><strong>Initialization</strong>:
<ul>
<li>Receives input data <code>X</code> and a set of pre-initialized parameters for each layer.</li>
<li>Initializes an empty list <code>caches</code> to store the cache values for later use.</li>
</ul></li>
<li><strong>Layer-wise Computation</strong>:
<ul>
<li>Iterates through each layer (from 1 to L-1) except the output layer.</li>
<li>Performs the [LINEAR -&gt; ACTIVATION] step, using ReLU activation for hidden layers.</li>
<li>Updates the current activation <code>A</code> and stores the computed cache for each layer in <code>caches</code>.</li>
</ul></li>
<li><strong>Output Layer Computation</strong>:
<ul>
<li>Executes a final [LINEAR -&gt; SIGMOID] step for the output layer to get the last post-activation value <code>AL</code>.</li>
<li>Appends this cache value to the <code>caches</code> list.</li>
</ul></li>
<li><strong>Validation and Return</strong>:
<ul>
<li>Asserts the shape of the final post-activation value <code>AL</code>.</li>
<li>Returns <code>AL</code>, representing the predicted values or probabilities, and <code>caches</code>, containing the cached values from each layer’s computation.</li>
</ul></li>
</ol>
<p>This function facilitates the sequential execution of the forward propagation steps, ensuring that each layer’s activation values are appropriately computed and stored for subsequent use during backward propagation.</p>
<div class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> L_model_forward(X, parameters):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">    X -- data, numpy array of shape (input size, number of examples)</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">    parameters -- output of initialize_parameters_deep()</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">    AL -- last post-activation value</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">    caches -- list of caches containing:</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    caches <span class="op">=</span> []</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> X</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> <span class="bu">len</span>(parameters) <span class="op">//</span> <span class="dv">2</span>                  <span class="co"># number of layers in the neural network</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Implement [LINEAR -&gt; RELU]*(L-1). Add "cache" to the "caches" list.</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, L):</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        A_prev <span class="op">=</span> A </span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        A, cache <span class="op">=</span> linear_activation_forward(A_prev, parameters[<span class="st">'W'</span> <span class="op">+</span> <span class="bu">str</span>(l)], parameters[<span class="st">'b'</span> <span class="op">+</span> <span class="bu">str</span>(l)], activation <span class="op">=</span> <span class="st">"relu"</span>)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        caches.append(cache)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Implement LINEAR -&gt; SIGMOID. Add "cache" to the "caches" list.</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    AL, cache <span class="op">=</span> linear_activation_forward(A, parameters[<span class="st">'W'</span> <span class="op">+</span> <span class="bu">str</span>(L)], parameters[<span class="st">'b'</span> <span class="op">+</span> <span class="bu">str</span>(L)], activation <span class="op">=</span> <span class="st">"sigmoid"</span>)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    caches.append(cache)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(AL.shape <span class="op">==</span> (<span class="dv">1</span>,X.shape[<span class="dv">1</span>]))</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> AL, caches</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="cost-computation" class="level2">
<h2 class="anchored" data-anchor-id="cost-computation">Cost Computation</h2>
<section id="cross-entropy-cost-function-implementation" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy-cost-function-implementation">Cross-Entropy Cost Function Implementation</h3>
<p>The function <code>compute_cost(AL, Y)</code> calculates the cross-entropy cost, an essential metric in evaluating the performance of a classification neural network. The implementation follows the cross-entropy formula defined by equation (7). It takes in the predicted probability vector <code>AL</code> and the true label vector <code>Y</code>, both of shape (1, number of examples).</p>
<p>The steps involved in this implementation are:</p>
<ol type="1">
<li><p><strong>Initialization</strong>: Obtain the number of examples <code>m</code> from the shape of <code>Y</code>.</p></li>
<li><p><strong>Compute Loss</strong>: Calculate the cross-entropy loss by utilizing the formula:</p>
<p><span class="math display">\[ \text{cost} = \frac{1}{m} \times \left( - \sum_{i=1}^{m} \left( Y \cdot \log(AL)^T + (1 - Y) \cdot \log(1 - AL)^T \right) \right) \]</span></p>
<p>This equation computes the loss by comparing the predicted probability vector <code>AL</code> against the true label vector <code>Y</code>. It sums the logarithmic loss for each example and averages it across all examples.</p></li>
<li><p><strong>Squeeze and Assert</strong>: To ensure the expected shape of the cost, the <code>np.squeeze()</code> function is applied, which transforms the shape to the expected scalar value. Additionally, an assertion check confirms that the shape of the cost matches the expected shape.</p></li>
<li><p><strong>Return</strong>: The computed cost value is returned to the calling function.</p></li>
</ol>
<p>This cost function is crucial in the training process of the neural network, as it quantifies the dissimilarity between the predicted and actual labels, guiding the optimization process towards minimizing this dissimilarity during model training.</p>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_cost(AL, Y):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Implement the cost function defined by equation (7).</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">    cost -- cross-entropy cost</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> Y.shape[<span class="dv">1</span>]</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute loss from aL and y.</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    cost <span class="op">=</span> (<span class="fl">1.</span><span class="op">/</span>m) <span class="op">*</span> (<span class="op">-</span>np.dot(Y,np.log(AL).T) <span class="op">-</span> np.dot(<span class="dv">1</span><span class="op">-</span>Y, np.log(<span class="dv">1</span><span class="op">-</span>AL).T))</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    cost <span class="op">=</span> np.squeeze(cost)      <span class="co"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(cost.shape <span class="op">==</span> ())</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="backward-propagation" class="level2">
<h2 class="anchored" data-anchor-id="backward-propagation">Backward Propagation</h2>
<section id="backward-propagation-derivation-and-implementation-for-neural-network-layers" class="level3">
<h3 class="anchored" data-anchor-id="backward-propagation-derivation-and-implementation-for-neural-network-layers">Backward Propagation Derivation and Implementation for Neural Network Layers</h3>
<p>This section focuses on the backward propagation process, a crucial step in training neural networks. It encompasses two main functions: <code>linear_backward</code> and <code>linear_activation_backward</code>. These functions form the backbone of the backpropagation algorithm for a single layer and a layer combined with an activation function, respectively.</p>
<ol type="1">
<li><strong><code>linear_backward</code> function:</strong>
<ul>
<li>Computes the gradients of the cost function concerning the layer’s parameters (weights and bias) and the activation output of the previous layer.</li>
<li>Utilizes the chain rule to compute gradients for the current layer’s weights, biases, and the activation output of the previous layer.</li>
<li>Derives gradients using the computed <code>dZ</code> (gradient of the cost with respect to the linear output) and the values cached during the forward propagation step.</li>
</ul></li>
</ol>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_backward(dZ, cache):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    A_prev, W, b <span class="op">=</span> cache</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> A_prev.shape[<span class="dv">1</span>]</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    dW <span class="op">=</span> <span class="fl">1.</span><span class="op">/</span>m <span class="op">*</span> np.dot(dZ,A_prev.T)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    db <span class="op">=</span> <span class="fl">1.</span><span class="op">/</span>m <span class="op">*</span> np.<span class="bu">sum</span>(dZ, axis <span class="op">=</span> <span class="dv">1</span>, keepdims <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    dA_prev <span class="op">=</span> np.dot(W.T,dZ)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> (dA_prev.shape <span class="op">==</span> A_prev.shape)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> (dW.shape <span class="op">==</span> W.shape)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> (db.shape <span class="op">==</span> b.shape)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dA_prev, dW, db</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li><strong><code>linear_activation_backward</code> function:</strong>
<ul>
<li>Implements the backward propagation for the combination of linear and activation layers.</li>
<li>Allows for flexibility in choosing different activation functions (sigmoid or ReLU) while performing backward propagation.</li>
<li>Combines the gradients calculated by the <code>linear_backward</code> function with the gradients derived from the activation function’s backward pass.</li>
</ul></li>
</ol>
<p>These functions collectively enable the computation of gradients at each layer of the neural network during backpropagation. Understanding this section is pivotal for comprehending how the model learns from its mistakes and adjusts its parameters to minimize the cost function.</p>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_activation_backward(dA, cache, activation):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">    dA -- post-activation gradient for current layer l </span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    linear_cache, activation_cache <span class="op">=</span> cache</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> activation <span class="op">==</span> <span class="st">"relu"</span>:</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>        dZ <span class="op">=</span> relu_backward(dA, activation_cache)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>        dA_prev, dW, db <span class="op">=</span> linear_backward(dZ, linear_cache)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> activation <span class="op">==</span> <span class="st">"sigmoid"</span>:</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>        dZ <span class="op">=</span> sigmoid_backward(dA, activation_cache)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>        dA_prev, dW, db <span class="op">=</span> linear_backward(dZ, linear_cache)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dA_prev, dW, db</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="gradient-descent-and-parameter-updates" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent-and-parameter-updates">Gradient Descent and Parameter Updates</h3>
<p>The code provided contains functions crucial for implementing the backpropagation algorithm in a neural network model. Specifically, it focuses on calculating gradients for parameters and updating these parameters using the gradient descent optimization technique.</p>
<ol type="1">
<li><strong>L_model_backward() Function</strong>:
<ul>
<li>This function executes the backward propagation for a neural network, specifically designed for the architecture of [LINEAR -&gt; RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID. It takes in the final output probability vector (AL), the true label vector (Y), and a list of caches generated during the forward propagation.</li>
<li>Initializes backpropagation by computing the derivative of the cost function with respect to the final output layer (dAL).</li>
<li>Then iterates through the layers in reverse order, applying the appropriate backward activations (sigmoid or ReLU) to calculate gradients for the parameters (dW and db) and activation of the previous layer (dA).</li>
<li>Finally, returns a dictionary containing the computed gradients.</li>
</ul></li>
<li><strong>update_parameters() Function</strong>:
<ul>
<li>This function implements the parameter update step using gradient descent.</li>
<li>It takes the current set of parameters, the gradients calculated from the backward propagation, and the learning rate as inputs.</li>
<li>Using a loop through the layers, it updates the weights (W) and biases (b) by subtracting the product of the learning rate and the corresponding gradients.</li>
<li>Returns the updated parameters for the neural network.</li>
</ul></li>
</ol>
<p>These functions collectively form the core of the training process in a neural network. They compute the gradients of the cost function with respect to the parameters, enabling iterative updates through gradient descent to optimize the network’s parameters for improved performance during training.</p>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> L_model_backward(AL, Y, caches):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">    caches -- list of caches containing:</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">                every cache of linear_activation_forward() with "relu" (there are (L-1) or them, indexes from 0 to L-2)</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co">                the cache of linear_activation_forward() with "sigmoid" (there is one, index L-1)</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co">    grads -- A dictionary with the gradients</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co">             grads["dA" + str(l)] = ... </span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="co">             grads["dW" + str(l)] = ...</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="co">             grads["db" + str(l)] = ... </span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> {}</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> <span class="bu">len</span>(caches) <span class="co"># the number of layers</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> AL.shape[<span class="dv">1</span>]</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> Y.reshape(AL.shape) <span class="co"># after this line, Y is the same shape as AL</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initializing the backpropagation</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    dAL <span class="op">=</span> <span class="op">-</span> (np.divide(Y, AL) <span class="op">-</span> np.divide(<span class="dv">1</span> <span class="op">-</span> Y, <span class="dv">1</span> <span class="op">-</span> AL))</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: "AL, Y, caches". Outputs: "grads["dAL"], grads["dWL"], grads["dbL"]</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>    current_cache <span class="op">=</span> caches[L<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>    grads[<span class="st">"dA"</span> <span class="op">+</span> <span class="bu">str</span>(L<span class="op">-</span><span class="dv">1</span>)], grads[<span class="st">"dW"</span> <span class="op">+</span> <span class="bu">str</span>(L)], grads[<span class="st">"db"</span> <span class="op">+</span> <span class="bu">str</span>(L)] <span class="op">=</span> linear_activation_backward(dAL, current_cache, activation <span class="op">=</span> <span class="st">"sigmoid"</span>)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(L<span class="op">-</span><span class="dv">1</span>)):</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>        current_cache <span class="op">=</span> caches[l]</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>        dA_prev_temp, dW_temp, db_temp <span class="op">=</span> linear_activation_backward(grads[<span class="st">"dA"</span> <span class="op">+</span> <span class="bu">str</span>(l <span class="op">+</span> <span class="dv">1</span>)], current_cache, activation <span class="op">=</span> <span class="st">"relu"</span>)</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        grads[<span class="st">"dA"</span> <span class="op">+</span> <span class="bu">str</span>(l)] <span class="op">=</span> dA_prev_temp</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>        grads[<span class="st">"dW"</span> <span class="op">+</span> <span class="bu">str</span>(l <span class="op">+</span> <span class="dv">1</span>)] <span class="op">=</span> dW_temp</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>        grads[<span class="st">"db"</span> <span class="op">+</span> <span class="bu">str</span>(l <span class="op">+</span> <span class="dv">1</span>)] <span class="op">=</span> db_temp</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> grads</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_parameters(parameters, grads, learning_rate):</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a><span class="co">    Update parameters using gradient descent</span></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a><span class="co">    parameters -- python dictionary containing your parameters </span></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a><span class="co">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a><span class="co">    parameters -- python dictionary containing your updated parameters </span></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a><span class="co">                  parameters["W" + str(l)] = ... </span></span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a><span class="co">                  parameters["b" + str(l)] = ...</span></span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> <span class="bu">len</span>(parameters) <span class="op">//</span> <span class="dv">2</span> <span class="co"># number of layers in the neural network</span></span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update rule for each parameter. Use a for loop.</span></span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(L):</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>        parameters[<span class="st">"W"</span> <span class="op">+</span> <span class="bu">str</span>(l<span class="op">+</span><span class="dv">1</span>)] <span class="op">=</span> parameters[<span class="st">"W"</span> <span class="op">+</span> <span class="bu">str</span>(l<span class="op">+</span><span class="dv">1</span>)] <span class="op">-</span> learning_rate <span class="op">*</span> grads[<span class="st">"dW"</span> <span class="op">+</span> <span class="bu">str</span>(l<span class="op">+</span><span class="dv">1</span>)]</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>        parameters[<span class="st">"b"</span> <span class="op">+</span> <span class="bu">str</span>(l<span class="op">+</span><span class="dv">1</span>)] <span class="op">=</span> parameters[<span class="st">"b"</span> <span class="op">+</span> <span class="bu">str</span>(l<span class="op">+</span><span class="dv">1</span>)] <span class="op">-</span> learning_rate <span class="op">*</span> grads[<span class="st">"db"</span> <span class="op">+</span> <span class="bu">str</span>(l<span class="op">+</span><span class="dv">1</span>)]</span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> parameters</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="model-training" class="level2">
<h2 class="anchored" data-anchor-id="model-training">Model Training</h2>
<section id="training-of-deep-neural-network-models" class="level3">
<h3 class="anchored" data-anchor-id="training-of-deep-neural-network-models">Training of Deep Neural Network Models</h3>
<p>The function <code>L_layer_model</code> implements the training process for both two-layer and L-layer neural network models. It employs a deep neural network architecture of the form: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID, allowing flexibility in defining the number of layers and their respective sizes.</p>
<p><strong>Key Components:</strong></p>
<ol type="1">
<li><p><strong>Initialization:</strong> The function initializes the parameters of the neural network using a deep initialization method via <code>initialize_parameters_deep</code>.</p></li>
<li><p><strong>Iterations (Gradient Descent):</strong> It performs a specified number of iterations (controlled by <code>num_iterations</code>) to optimize the parameters. During each iteration:</p>
<ul>
<li>Forward Propagation: The forward pass computes the activations (AL) and caches through the neural network layers using <code>L_model_forward</code>.</li>
<li>Cost Computation: The cost function is computed to evaluate the performance of the network through <code>compute_cost</code>.</li>
<li>Backward Propagation: The gradients are calculated using backpropagation via <code>L_model_backward</code>.</li>
<li>Parameter Update: The parameters are updated using the calculated gradients and a specified learning rate through <code>update_parameters</code>.</li>
</ul></li>
<li><p><strong>Cost Tracking:</strong> It maintains a record of the cost after every 100 iterations (or the final iteration if specified) and appends it to the <code>costs</code> list.</p></li>
<li><p><strong>Print Cost (Optional):</strong> The parameter <code>print_cost</code> enables the display of the cost value at specified intervals (every 100 iterations) for tracking the convergence of the model during training.</p></li>
</ol>
<p><strong>Returns:</strong></p>
<p>The function returns the optimized parameters learned during training (<code>parameters</code>) and a list of costs (<code>costs</code>) tracked over the iterations, which can be useful for visualizing the learning curve and assessing convergence.</p>
<p><strong>Usage:</strong></p>
<p>This function is versatile, allowing the training of deep neural networks by defining the number of layers (<code>layers_dims</code>), input data (<code>X</code>), true labels (<code>Y</code>), learning rate (<code>learning_rate</code>), and the number of optimization iterations (<code>num_iterations</code>). The <code>print_cost</code> flag controls whether the cost is printed during training iterations, providing flexibility for monitoring training progress.</p>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> L_layer_model(X, Y, layers_dims, learning_rate<span class="op">=</span><span class="fl">0.0075</span>, num_iterations<span class="op">=</span><span class="dv">3000</span>, print_cost<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co">    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co">    learning_rate -- learning rate of the gradient descent update rule</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co">    num_iterations -- number of iterations of the optimization loop</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co">    print_cost -- if True, it prints the cost every 100 steps</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    np.random.seed(<span class="dv">1</span>)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>    costs <span class="op">=</span> []  <span class="co"># keep track of cost</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Parameters initialization</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>    parameters <span class="op">=</span> initialize_parameters_deep(layers_dims)</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop (gradient descent)</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        AL, caches <span class="op">=</span> L_model_forward(X, parameters)</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute cost</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>        cost <span class="op">=</span> compute_cost(AL, Y)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward propagation</span></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        grads <span class="op">=</span> L_model_backward(AL, Y, caches)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update parameters</span></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>        parameters <span class="op">=</span> update_parameters(parameters, grads, learning_rate)</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print the cost every 100 iterations</span></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> print_cost <span class="kw">and</span> (i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> i <span class="op">==</span> num_iterations <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Cost after iteration </span><span class="sc">{}</span><span class="st">: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(i, np.squeeze(cost)))</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> i <span class="op">==</span> num_iterations <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>            costs.append(cost)</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> parameters, costs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="gradient-descent-and-parameter-updates-1" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent-and-parameter-updates-1">Gradient Descent and Parameter Updates</h3>
<p>This section involves the iterative process of optimizing the neural network through gradient descent and updating the model parameters. The <code>L_layer_model</code> function, called with specified parameters like <code>num_iterations = 2500</code> and <code>print_cost = True</code>, encapsulates the core of this process.</p>
<ul>
<li><p><strong>Gradient Descent Iterations:</strong> The function initiates a loop for a specified number of iterations (<code>num_iterations = 2500</code>) where the model undergoes forward and backward propagation to compute the gradients and update the parameters.</p></li>
<li><p><strong>Cost Evaluation:</strong> During each iteration, the cost (or loss) is computed and printed if <code>print_cost</code> is set to <code>True</code>. This allows monitoring the model’s learning progress over the iterations.</p></li>
<li><p><strong>Parameter Updates:</strong> Through backpropagation, gradients are computed for each layer, and the parameters (weights and biases) are updated using a learning rate and the calculated gradients to minimize the cost function.</p></li>
</ul>
<p>This section represents the heart of training the neural network, where the model learns from the training data by adjusting its parameters iteratively to minimize the cost, eventually improving its predictive capability.</p>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>parameters, costs <span class="op">=</span> L_layer_model(train_x, train_y, layers_dims, num_iterations <span class="op">=</span> <span class="dv">2500</span>, print_cost <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cost after iteration 0: 0.7717493284237686
Cost after iteration 100: 0.6720534400822914
Cost after iteration 200: 0.6482632048575212
Cost after iteration 300: 0.6115068816101354
Cost after iteration 400: 0.5670473268366111
Cost after iteration 500: 0.5401376634547801
Cost after iteration 600: 0.5279299569455267
Cost after iteration 700: 0.4654773771766851
Cost after iteration 800: 0.36912585249592794
Cost after iteration 900: 0.39174697434805344
Cost after iteration 1000: 0.3151869888600617
Cost after iteration 1100: 0.2726998441789385
Cost after iteration 1200: 0.23741853400268134
Cost after iteration 1300: 0.19960120532208644
Cost after iteration 1400: 0.18926300388463305
Cost after iteration 1500: 0.16118854665827748
Cost after iteration 1600: 0.14821389662363316
Cost after iteration 1700: 0.13777487812972944
Cost after iteration 1800: 0.12974017549190123
Cost after iteration 1900: 0.12122535068005211
Cost after iteration 2000: 0.11382060668633713
Cost after iteration 2100: 0.10783928526254132
Cost after iteration 2200: 0.10285466069352679
Cost after iteration 2300: 0.10089745445261787
Cost after iteration 2400: 0.09287821526472397
Cost after iteration 2499: 0.088439943441702</code></pre>
</div>
</div>
<p><strong>Observations:</strong></p>
<ul>
<li><strong>Cost Decrease:</strong> The cost reduces progressively as the number of iterations increases. This reduction indicates that the model is learning and optimizing its parameters to better fit the training data.</li>
<li><strong>Convergence:</strong> Initially, the cost starts relatively high at <code>0.7717</code> and consistently decreases with each iteration. As the iterations progress, the rate of reduction in the cost diminishes, implying that the model is converging towards an optimal solution.</li>
<li><strong>Stabilization:</strong> After around <code>2000</code> iterations, the cost reduction slows down considerably, with smaller decreases in subsequent iterations. This signifies that the model’s improvement becomes marginal, indicating it is approaching convergence or an optimal solution.</li>
</ul>
<p>Overall, the output demonstrates the iterative nature of the training process, showing how the neural network learns and adjusts its parameters to minimize the cost function, thereby enhancing its predictive capability on the training data.</p>
</section>
</section>
<section id="model-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="model-evaluation">Model Evaluation</h2>
<section id="model-performance-evaluation-and-accuracy-assessment" class="level3">
<h3 class="anchored" data-anchor-id="model-performance-evaluation-and-accuracy-assessment">Model Performance Evaluation and Accuracy Assessment</h3>
<p>This section evaluates the performance of the trained L-layer neural network model by assessing its prediction accuracy on both the training and test datasets. The function <code>predict()</code> is utilized to make predictions on the provided datasets (<code>train_x</code> and <code>test_x</code>) using the trained model parameters.</p>
<p>The function performs the following steps: - Takes the dataset examples (<code>X</code>) and corresponding labels (<code>y</code>) as inputs along with the trained model parameters. - Utilizes forward propagation through the neural network to obtain predictions (<code>p</code>) for the given dataset <code>X</code>. - Converts the raw probabilities (<code>probas</code>) into binary predictions (0 or 1) based on a threshold of 0.5. - Calculates and displays the accuracy of the predictions by comparing them with the true labels (<code>y</code>).</p>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(X, y, parameters):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co">    This function is used to predict the results of a  L-layer neural network.</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co">    X -- data set of examples you would like to label</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co">    parameters -- parameters of the trained model</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co">    p -- predictions for the given dataset X</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(parameters) <span class="op">//</span> <span class="dv">2</span> <span class="co"># number of layers in the neural network</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> np.zeros((<span class="dv">1</span>,m))</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward propagation</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    probas, caches <span class="op">=</span> L_model_forward(X, parameters)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convert probas to 0/1 predictions</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, probas.shape[<span class="dv">1</span>]):</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> probas[<span class="dv">0</span>,i] <span class="op">&gt;</span> <span class="fl">0.5</span>:</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>            p[<span class="dv">0</span>,i] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>            p[<span class="dv">0</span>,i] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Accuracy: "</span>  <span class="op">+</span> <span class="bu">str</span>(np.<span class="bu">sum</span>((p <span class="op">==</span> y)<span class="op">/</span>m)))</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> p</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>pred_train <span class="op">=</span> predict(train_x, train_y, parameters)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.9856459330143539</code></pre>
</div>
</div>
<div class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>pred_test <span class="op">=</span> predict(test_x, test_y, parameters)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.8</code></pre>
</div>
</div>
<p>The reported accuracy for the training dataset is approximately 98.56%, while the accuracy for the test dataset stands at 80.0%. Assessing the accuracy on both datasets provides insights into the model’s performance on seen and unseen data, indicating its capability to generalize beyond the training set.</p>
</section>
</section>
<section id="results-visualization" class="level2">
<h2 class="anchored" data-anchor-id="results-visualization">Results Visualization</h2>
<section id="cost-evolution-during-training-iterations" class="level3">
<h3 class="anchored" data-anchor-id="cost-evolution-during-training-iterations">Cost Evolution During Training Iterations</h3>
<p>The visualization depicts the evolution of the cost or loss function over training iterations. It is generated using the <code>plot_costs</code> function, which takes in the costs array as input. The y-axis represents the cost value, while the x-axis shows the iterations in multiples of hundreds.</p>
<p>The plot demonstrates the decreasing trend of the cost function with increasing iterations, indicating the model’s learning progress. It’s evident from the displayed cost values after each iteration that the cost gradually decreases. Initially, the cost is relatively high, reflecting the model’s higher error rate, but it decreases consistently over training epochs. As the iterations progress, the cost approaches a lower value, signifying the model’s improvement in minimizing errors and getting closer to optimal parameters.</p>
<p>The plotted curve illustrates how the cost decreases over time, providing insight into the model’s learning behavior and convergence towards better performance.</p>
<div class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_costs(costs, learning_rate<span class="op">=</span><span class="fl">0.0075</span>):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    plt.plot(np.squeeze(costs))</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'cost'</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'iterations (per hundreds)'</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Learning rate ="</span> <span class="op">+</span> <span class="bu">str</span>(learning_rate))</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>plot_costs(costs, learning_rate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Cat_classifier_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="visualizing-misclassified-images" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-misclassified-images">Visualizing Misclassified Images</h3>
<p>This section utilizes a function named <code>print_mislabeled_images()</code> to visually represent misclassified images. The function takes parameters including the class labels, dataset (<code>X</code>), true labels (<code>y</code>), and predicted labels (<code>p</code>). It aims to plot and display images where the predictions made by the model differ from the true labels.</p>
<p>The code identifies and extracts misclassified indices, highlighting cases where the sum of predicted and true labels is equal to 1. It then generates visualizations for these misclassified images by iterating through the identified indices. The plot showcases the image, its predicted class label, and the actual class label.</p>
<p>In this specific instance, the function displays 10 misclassified images, enabling an insightful view into instances where the model’s predictions did not align with the ground truth labels. The visualization aids in understanding the nature of misclassifications and potentially identifying patterns or challenges within the model’s performance.</p>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_mislabeled_images(classes, X, y, p):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Plots images where predictions and truth were different.</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co">    X -- dataset</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co">    y -- true labels</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co">    p -- predictions</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> p <span class="op">+</span> y</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    mislabeled_indices <span class="op">=</span> np.asarray(np.where(a <span class="op">==</span> <span class="dv">1</span>))</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> (<span class="fl">40.0</span>, <span class="fl">40.0</span>) <span class="co"># set default size of plots</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    num_images <span class="op">=</span> <span class="bu">len</span>(mislabeled_indices[<span class="dv">0</span>])</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_images):</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>        index <span class="op">=</span> mislabeled_indices[<span class="dv">1</span>][i]</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">2</span>, num_images, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>        plt.imshow(X[:,index].reshape(<span class="dv">64</span>,<span class="dv">64</span>,<span class="dv">3</span>), interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"Prediction: "</span> <span class="op">+</span> classes[<span class="bu">int</span>(p[<span class="dv">0</span>,index])].decode(<span class="st">"utf-8"</span>) <span class="op">+</span> <span class="st">" </span><span class="ch">\n</span><span class="st"> Class: "</span> <span class="op">+</span> classes[y[<span class="dv">0</span>,index]].decode(<span class="st">"utf-8"</span>))</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>print_mislabeled_images(classes, test_x, test_y, pred_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Cat_classifier_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="conclusion-assessing-the-cat-image-classification-neural-network" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-assessing-the-cat-image-classification-neural-network">Conclusion: Assessing the Cat Image Classification Neural Network</h2>
<p>The development of the cat image classification neural network showcases the application of deep learning techniques for image recognition tasks. Throughout this project, several key aspects have been addressed:</p>
<p><strong>Model Architecture and Training:</strong> The notebook implemented an L-layer neural network from scratch using numpy and scipy. The architecture included linear and activation functions (ReLU and sigmoid) and underwent iterative training to optimize parameters for better accuracy.</p>
<p><strong>Performance Evaluation:</strong> The trained models were evaluated on both the training and test sets. The assessment included metrics like accuracy, cost, and visual representation of incorrectly classified images. These evaluations provided insights into the model’s ability to distinguish cat images from non-cat images.</p>
<p><strong>Key Findings:</strong> The models exhibited varying levels of performance. The L-layer network demonstrated improved accuracy and better representation of features.</p>
<p><strong>Challenges and Future Directions:</strong> Despite the successful implementation, challenges like overfitting, optimization convergence, and computational intensity were encountered. To further enhance the model’s performance, regularization techniques, hyperparameter tuning, and exploring more complex architectures could be considered.</p>
<p><strong>Application and Relevance:</strong> The cat image classification neural network exemplifies the practical application of machine learning in real-world scenarios, specifically in image recognition tasks. The skills acquired and lessons learned from this project lay a solid foundation for tackling more complex image classification problems.</p>
<p><strong>Final Thoughts:</strong> Building a neural network from scratch not only provided a deep understanding of its inner workings but also emphasized the significance of data preprocessing, model architecture, and iterative optimization in achieving robust and accurate predictions.</p>
<p>In essence, this project not only successfully classified cat images but also served as a stepping stone towards understanding and refining neural networks for diverse image recognition applications.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>
[
  {
    "objectID": "Cat_classifier.html",
    "href": "Cat_classifier.html",
    "title": "Building a Cat Image Classifier using Neural Networks from Scratch",
    "section": "",
    "text": "Repo link : https://github.com/SherryS997/cat-image-classification-neural-network-numpy-scipy"
  },
  {
    "objectID": "Cat_classifier.html#introduction",
    "href": "Cat_classifier.html#introduction",
    "title": "Building a Cat Image Classifier using Neural Networks from Scratch",
    "section": "Introduction",
    "text": "Introduction\nThe following Jupyter Notebook encapsulates the step-by-step creation of a cat image classifier using Neural Networks, entirely coded from scratch using Python libraries like NumPy, SciPy, and Matplotlib. This project aims to distinguish cat images from non-cat images using machine learning techniques implemented through a neural network architecture.\nThe notebook begins by loading the necessary libraries and datasets containing both training and test images of cats and non-cats. It further explores the dataset’s structure, dimensions, and the process of standardizing image data to be used in the neural network.\nThe construction of the neural network includes defining activation functions (sigmoid and ReLU), initializing parameters, implementing forward and backward propagation, and updating parameters via gradient descent. The model’s performance is assessed using various functions to compute the cost, make predictions, and visualize mislabeled images.\nAdditionally, the notebook contains a function to predict images outside the dataset, enabling users to test the model’s classification capabilities on their images.\n\nLibrary Imports and Environment Setup\nThis section initializes the notebook by importing essential libraries and configuring the environment. It includes importing standard libraries such as NumPy, SciPy, Matplotlib, and PIL (Python Imaging Library). Additionally, the section sets up the notebook environment by configuring parameters for Matplotlib plots, ensuring consistent visualization settings throughout the notebook. This step is crucial as it prepares the groundwork for subsequent data handling, model building, and result visualization within the notebook.\n\nimport time\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\nnp.random.seed(1)"
  },
  {
    "objectID": "Cat_classifier.html#data-loading-and-exploration",
    "href": "Cat_classifier.html#data-loading-and-exploration",
    "title": "Building a Cat Image Classifier using Neural Networks from Scratch",
    "section": "Data Loading and Exploration",
    "text": "Data Loading and Exploration\n\nDataset Loading and Preprocessing\nThis section of the code focuses on loading the cat image dataset, which includes both training and test sets, from .h5 files. It reads the data and relevant labels, separating them into training features (train_x_orig) and labels (train_y), and test features (test_x_orig) and labels (test_y). Additionally, it retrieves the classes/categories from the dataset.\nThe data is loaded using the h5py library and converted into NumPy arrays for further processing. Reshaping and organizing the label data ensure compatibility with subsequent operations. This step is crucial as it lays the foundation for subsequent data preprocessing and model development stages.\n\ntrain_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\ntrain_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\ntrain_y = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\ntest_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\ntest_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\ntest_y = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\nclasses = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n\ntrain_y = train_y.reshape((1, train_y.shape[0]))\ntest_y = test_y.reshape((1, test_y.shape[0]))\n\n\n\nVisualizing Dataset Samples and Labels\nThis section presents a visualization of the dataset samples along with their respective labels. Using matplotlib, it displays a specific image from the training set identified by its index. Additionally, the label associated with the image is shown, providing clarity about the class it represents. In this instance, the output displays an example image labeled as a non-cat (y = 0), portraying a picture that corresponds to a hummingbird.\n\n# Example of a picture\nindex = 10\nplt.imshow(train_x_orig[index])\nprint (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")\n\ny = 0. It's a non-cat picture.\n\n\n\n\n\n\n\nDataset Overview and Structure\nThis section provides an insight into the dataset statistics, presenting essential information such as the number of training and testing examples, along with details regarding the dimensions and size of each image. It displays the number of training and testing examples, specifying the dimensions and shape of the image arrays. This exploration helps understand the dataset’s structure and prepares for preprocessing and model development.\n\n# Explore your dataset \nm_train = train_x_orig.shape[0]\nnum_px = train_x_orig.shape[1]\nm_test = test_x_orig.shape[0]\n\nprint (\"Number of training examples: \" + str(m_train))\nprint (\"Number of testing examples: \" + str(m_test))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_x_orig shape: \" + str(train_x_orig.shape))\nprint (\"train_y shape: \" + str(train_y.shape))\nprint (\"test_x_orig shape: \" + str(test_x_orig.shape))\nprint (\"test_y shape: \" + str(test_y.shape))\n\nNumber of training examples: 209\nNumber of testing examples: 50\nEach image is of size: (64, 64, 3)\ntrain_x_orig shape: (209, 64, 64, 3)\ntrain_y shape: (1, 209)\ntest_x_orig shape: (50, 64, 64, 3)\ntest_y shape: (1, 50)\n\n\nThe output from the code snippet provides crucial insights into the dataset structure and composition:\n\nNumber of training examples: The dataset comprises 209 training examples, which serves as the data used for training the classification model.\nNumber of testing examples: There are 50 testing examples, utilized to evaluate the trained model’s performance on unseen data.\nImage size: Each image in the dataset has a dimension of (64, 64, 3), indicating that the images are 64 pixels in height and width with three color channels (RGB).\nTrain and test set shapes: The training set, denoted by train_x_orig, consists of 209 images, each with a size of (64, 64, 3). The associated training labels (train_y) have a shape of (1, 209). Similarly, the test set (test_x_orig) comprises 50 images with the same dimensions as the training images, and the corresponding test labels (test_y) possess a shape of (1, 50).\n\nUnderstanding these dataset statistics is vital for various tasks, such as data preprocessing, designing the neural network architecture, and assessing the model’s performance during training and testing phases."
  },
  {
    "objectID": "Cat_classifier.html#data-preprocessing",
    "href": "Cat_classifier.html#data-preprocessing",
    "title": "Building a Cat Image Classifier using Neural Networks from Scratch",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nData Reshaping and Normalization\nThis section involves the preprocessing steps applied to the dataset before feeding it into the neural network model. It includes two key operations:\n\nReshaping: The code snippet reshapes the training and test examples. Using the reshape() function, the images are flattened to a 1D array while preserving the number of samples. This operation is crucial as it transforms the multi-dimensional image data into a format suitable for further processing.\nNormalization: Normalization is performed to standardize the pixel values of the images. The pixel values, initially ranging from 0 to 255, are scaled to be between 0 and 1 by dividing each pixel value by 255. Normalizing the data helps in achieving uniformity and stability during training, aiding the neural network to learn effectively without being skewed by varying pixel ranges. The standardized data is denoted as train_x and test_x.\n\nThese preprocessing steps are essential for preparing the data before training the neural network, ensuring that the model learns effectively and efficiently from the input images.\n\n# Reshape the training and test examples \ntrain_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\ntest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n\n# Standardize data to have feature values between 0 and 1.\ntrain_x = train_x_flatten/255.\ntest_x = test_x_flatten/255.\n\nprint (\"train_x's shape: \" + str(train_x.shape))\nprint (\"test_x's shape: \" + str(test_x.shape))\n\ntrain_x's shape: (12288, 209)\ntest_x's shape: (12288, 50)"
  },
  {
    "objectID": "Cat_classifier.html#neural-network-architecture",
    "href": "Cat_classifier.html#neural-network-architecture",
    "title": "Building a Cat Image Classifier using Neural Networks from Scratch",
    "section": "Neural Network Architecture",
    "text": "Neural Network Architecture\n\nNeural Network Architecture Definition\nDescription: This section defines the architecture of the neural network based on specific constants and parameters. It establishes the structure of the model by setting the dimensions of each layer, including the input layer, hidden layers, and output layer. The constants n_x, n_h, and n_y represent the sizes of the input features, hidden layers, and output respectively. Additionally, the layers_dims list specifies the dimensions of each layer in a 4-layer neural network, providing insight into the overall structure of the model. The learning_rate parameter, crucial for optimization, is also initialized here.\n\n### CONSTANTS DEFINING THE MODEL ####\nn_x = num_px * num_px * 3 \nn_h = 7\nn_y = 1\nlayers_dims = [12288, 20, 7, 5, 1] #  4-layer model\nlearning_rate = 0.0075\n\n\n\nDeep Neural Network Parameter Initialization\nThe function “initialize_parameters_deep” sets up the initial values for the weights (W) and biases (b) of a deep neural network with multiple layers. It takes in an array (layer_dims) containing the dimensions of each layer in the network.\nThis function initializes the parameters for each layer (W1, b1, …, WL, bL) as a Python dictionary (parameters). For each layer l, it generates random weights (Wl) of shape (layer_dims[l], layer_dims[l-1]) using a Gaussian distribution and scales them by np.sqrt(layer_dims[l-1]). The biases (bl) are initialized as zero vectors of shape (layer_dims[l], 1).\nThe purpose of this initialization is to provide suitable starting values for the weights and biases, aiding in the convergence of the neural network during training. The appropriate initialization helps prevent issues like vanishing/exploding gradients and contributes to better learning in the subsequent training phases of the network.\n\ndef initialize_parameters_deep(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    \n    np.random.seed(1)\n    parameters = {}\n    L = len(layer_dims)            # number of layers in the network\n\n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n        \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n\n        \n    return parameters\n\n\n\nActivation Functions Implementation (Sigmoid and ReLU)\nThis section in the code comprises the implementation of two fundamental activation functions used in neural networks: Sigmoid and Rectified Linear Unit (ReLU).\n\nSigmoid Activation Function:\n\nDefinition: The sigmoid activation function is implemented to squash the input values between 0 and 1, facilitating non-linear transformations in neural networks.\nImplementation: The sigmoid function takes in any numpy array Z and computes the output A using the formula A = 1 / (1 + np.exp(-Z)). It returns the computed A and also caches the input Z for efficient backpropagation during the training process.\nBackward Propagation: The sigmoid_backward function computes the gradient of the cost with respect to Z during backpropagation. It utilizes the cached Z and the incoming gradient dA to compute dZ, which is essential for updating weights in the network.\n\nReLU (Rectified Linear Unit) Activation Function:\n\nDefinition: ReLU is a widely used activation function that introduces non-linearity by setting all negative values to zero and leaving positive values unchanged.\nImplementation: The relu function takes the output Z from a linear layer and computes the post-activation output A using A = np.maximum(0, Z). It also caches the input Z for efficient backward pass computation.\nBackward Propagation: The relu_backward function calculates the gradient of the cost with respect to Z for a single ReLU unit. It utilizes the cached Z and incoming gradient dA to compute dZ. It ensures that for Z &lt;= 0, the gradient dZ is set to zero to handle the non-differentiability at Z = 0.\n\n\nThese activation functions, Sigmoid and ReLU, play pivotal roles in introducing non-linearities within neural networks, aiding in learning complex patterns and enhancing the network’s representational capacity during training.\n\ndef sigmoid(Z):\n    \"\"\"\n    Implements the sigmoid activation in numpy\n    \n    Arguments:\n    Z -- numpy array of any shape\n    \n    Returns:\n    A -- output of sigmoid(z), same shape as Z\n    cache -- returns Z as well, useful during backpropagation\n    \"\"\"\n    \n    A = 1/(1+np.exp(-Z))\n    cache = Z\n    \n    return A, cache\n\ndef relu(Z):\n    \"\"\"\n    Implement the RELU function.\n\n    Arguments:\n    Z -- Output of the linear layer, of any shape\n\n    Returns:\n    A -- Post-activation parameter, of the same shape as Z\n    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    \n    A = np.maximum(0,Z)\n    \n    assert(A.shape == Z.shape)\n    \n    cache = Z \n    return A, cache\n\n\ndef relu_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single RELU unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    \n    Z = cache\n    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n    \n    # When z &lt;= 0, you should set dz to 0 as well. \n    dZ[Z &lt;= 0] = 0\n    \n    assert (dZ.shape == Z.shape)\n    \n    return dZ\n\ndef sigmoid_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single SIGMOID unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    \n    Z = cache\n    \n    s = 1/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)\n    \n    assert (dZ.shape == Z.shape)\n    \n    return dZ"
  },
  {
    "objectID": "Cat_classifier.html#forward-propagation",
    "href": "Cat_classifier.html#forward-propagation",
    "title": "Building a Cat Image Classifier using Neural Networks from Scratch",
    "section": "Forward Propagation",
    "text": "Forward Propagation\n\nForward Propagation Implementation for Neural Network Layers\nThe code snippet presents the implementation of forward propagation for a neural network’s layers, encompassing both linear transformation and activation functions. The following functions are integral components of this forward propagation process:\n\nlinear_forward Function:\n\nThis function computes the linear part of a layer’s forward propagation.\nIt calculates the pre-activation parameter Z using the weights W, activations A from the previous layer, and bias b.\nThe resulting pre-activation parameter Z is fundamental for subsequent activation functions.\nThe calculated values are cached in a dictionary (cache) containing A, W, and b for efficient computation during the backward pass.\n\n\n\ndef linear_forward(A, W, b):\n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter \n    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    \n    Z = W.dot(A) + b\n    \n    assert(Z.shape == (W.shape[0], A.shape[1]))\n    cache = (A, W, b)\n    \n    return Z, cache\n\n\nlinear_activation_forward Function:\n\nThis function implements the complete forward propagation for the layer, combining the linear transformation and an activation function (sigmoid or ReLU).\nDepending on the specified activation function, it computes the post-activation value A using the pre-activation Z obtained from the linear_forward step.\nThe resultant post-activation value A along with the linear and activation function caches (linear_cache and activation_cache) are stored in a dictionary (cache). These caches are crucial for efficient computation during the subsequent backward pass.\n\n\nBoth functions work in tandem to compute the forward pass through a layer, incorporating linear transformations and different activation functions based on the specified requirements (sigmoid or ReLU). The caches obtained during this process facilitate the backward propagation for updating the neural network’s parameters during training.\n\ndef linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value \n    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently\n    \"\"\"\n    \n    if activation == \"sigmoid\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n    \n    elif activation == \"relu\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n    \n    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n    cache = (linear_cache, activation_cache)\n\n    return A, cache\n\n\n\nForward Propagation in L-layer Neural Networks\nThe function L_model_forward orchestrates the forward propagation process within an L-layer neural network. It executes a sequence of [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computations. Here’s how it works:\n\nInitialization:\n\nReceives input data X and a set of pre-initialized parameters for each layer.\nInitializes an empty list caches to store the cache values for later use.\n\nLayer-wise Computation:\n\nIterates through each layer (from 1 to L-1) except the output layer.\nPerforms the [LINEAR -&gt; ACTIVATION] step, using ReLU activation for hidden layers.\nUpdates the current activation A and stores the computed cache for each layer in caches.\n\nOutput Layer Computation:\n\nExecutes a final [LINEAR -&gt; SIGMOID] step for the output layer to get the last post-activation value AL.\nAppends this cache value to the caches list.\n\nValidation and Return:\n\nAsserts the shape of the final post-activation value AL.\nReturns AL, representing the predicted values or probabilities, and caches, containing the cached values from each layer’s computation.\n\n\nThis function facilitates the sequential execution of the forward propagation steps, ensuring that each layer’s activation values are appropriately computed and stored for subsequent use during backward propagation.\n\ndef L_model_forward(X, parameters):\n    \"\"\"\n    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation\n    \n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters_deep()\n    \n    Returns:\n    AL -- last post-activation value\n    caches -- list of caches containing:\n                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n    \"\"\"\n\n    caches = []\n    A = X\n    L = len(parameters) // 2                  # number of layers in the neural network\n    \n    # Implement [LINEAR -&gt; RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    for l in range(1, L):\n        A_prev = A \n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n        caches.append(cache)\n    \n    # Implement LINEAR -&gt; SIGMOID. Add \"cache\" to the \"caches\" list.\n    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n    caches.append(cache)\n    \n    assert(AL.shape == (1,X.shape[1]))\n            \n    return AL, caches"
  },
  {
    "objectID": "Cat_classifier.html#cost-computation",
    "href": "Cat_classifier.html#cost-computation",
    "title": "Building a Cat Image Classifier using Neural Networks from Scratch",
    "section": "Cost Computation",
    "text": "Cost Computation\n\nCross-Entropy Cost Function Implementation\nThe function compute_cost(AL, Y) calculates the cross-entropy cost, an essential metric in evaluating the performance of a classification neural network. The implementation follows the cross-entropy formula defined by equation (7). It takes in the predicted probability vector AL and the true label vector Y, both of shape (1, number of examples).\nThe steps involved in this implementation are:\n\nInitialization: Obtain the number of examples m from the shape of Y.\nCompute Loss: Calculate the cross-entropy loss by utilizing the formula:\n\\[ \\text{cost} = \\frac{1}{m} \\times \\left( - \\sum_{i=1}^{m} \\left( Y \\cdot \\log(AL)^T + (1 - Y) \\cdot \\log(1 - AL)^T \\right) \\right) \\]\nThis equation computes the loss by comparing the predicted probability vector AL against the true label vector Y. It sums the logarithmic loss for each example and averages it across all examples.\nSqueeze and Assert: To ensure the expected shape of the cost, the np.squeeze() function is applied, which transforms the shape to the expected scalar value. Additionally, an assertion check confirms that the shape of the cost matches the expected shape.\nReturn: The computed cost value is returned to the calling function.\n\nThis cost function is crucial in the training process of the neural network, as it quantifies the dissimilarity between the predicted and actual labels, guiding the optimization process towards minimizing this dissimilarity during model training.\n\ndef compute_cost(AL, Y):\n    \"\"\"\n    Implement the cost function defined by equation (7).\n\n    Arguments:\n    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost\n    \"\"\"\n    \n    m = Y.shape[1]\n\n    # Compute loss from aL and y.\n    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n    \n    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n    assert(cost.shape == ())\n    \n    return cost"
  },
  {
    "objectID": "Cat_classifier.html#backward-propagation",
    "href": "Cat_classifier.html#backward-propagation",
    "title": "Building a Cat Image Classifier using Neural Networks from Scratch",
    "section": "Backward Propagation",
    "text": "Backward Propagation\n\nBackward Propagation Derivation and Implementation for Neural Network Layers\nThis section focuses on the backward propagation process, a crucial step in training neural networks. It encompasses two main functions: linear_backward and linear_activation_backward. These functions form the backbone of the backpropagation algorithm for a single layer and a layer combined with an activation function, respectively.\n\nlinear_backward function:\n\nComputes the gradients of the cost function concerning the layer’s parameters (weights and bias) and the activation output of the previous layer.\nUtilizes the chain rule to compute gradients for the current layer’s weights, biases, and the activation output of the previous layer.\nDerives gradients using the computed dZ (gradient of the cost with respect to the linear output) and the values cached during the forward propagation step.\n\n\n\ndef linear_backward(dZ, cache):\n    \"\"\"\n    Implement the linear portion of backward propagation for a single layer (layer l)\n\n    Arguments:\n    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    dW = 1./m * np.dot(dZ,A_prev.T)\n    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n    dA_prev = np.dot(W.T,dZ)\n    \n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    \n    return dA_prev, dW, db\n\n\nlinear_activation_backward function:\n\nImplements the backward propagation for the combination of linear and activation layers.\nAllows for flexibility in choosing different activation functions (sigmoid or ReLU) while performing backward propagation.\nCombines the gradients calculated by the linear_backward function with the gradients derived from the activation function’s backward pass.\n\n\nThese functions collectively enable the computation of gradients at each layer of the neural network during backpropagation. Understanding this section is pivotal for comprehending how the model learns from its mistakes and adjusts its parameters to minimize the cost function.\n\ndef linear_activation_backward(dA, cache, activation):\n    \"\"\"\n    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.\n    \n    Arguments:\n    dA -- post-activation gradient for current layer l \n    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n    \n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    linear_cache, activation_cache = cache\n    \n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        \n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    \n    return dA_prev, dW, db\n\n\n\nGradient Descent and Parameter Updates\nThe code provided contains functions crucial for implementing the backpropagation algorithm in a neural network model. Specifically, it focuses on calculating gradients for parameters and updating these parameters using the gradient descent optimization technique.\n\nL_model_backward() Function:\n\nThis function executes the backward propagation for a neural network, specifically designed for the architecture of [LINEAR -&gt; RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID. It takes in the final output probability vector (AL), the true label vector (Y), and a list of caches generated during the forward propagation.\nInitializes backpropagation by computing the derivative of the cost function with respect to the final output layer (dAL).\nThen iterates through the layers in reverse order, applying the appropriate backward activations (sigmoid or ReLU) to calculate gradients for the parameters (dW and db) and activation of the previous layer (dA).\nFinally, returns a dictionary containing the computed gradients.\n\nupdate_parameters() Function:\n\nThis function implements the parameter update step using gradient descent.\nIt takes the current set of parameters, the gradients calculated from the backward propagation, and the learning rate as inputs.\nUsing a loop through the layers, it updates the weights (W) and biases (b) by subtracting the product of the learning rate and the corresponding gradients.\nReturns the updated parameters for the neural network.\n\n\nThese functions collectively form the core of the training process in a neural network. They compute the gradients of the cost function with respect to the parameters, enabling iterative updates through gradient descent to optimize the network’s parameters for improved performance during training.\n\ndef L_model_backward(AL, Y, caches):\n    \"\"\"\n    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group\n    \n    Arguments:\n    AL -- probability vector, output of the forward propagation (L_model_forward())\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n    \n    Returns:\n    grads -- A dictionary with the gradients\n             grads[\"dA\" + str(l)] = ... \n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ... \n    \"\"\"\n    grads = {}\n    L = len(caches) # the number of layers\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n    \n    # Initializing the backpropagation\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    \n    # Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n    current_cache = caches[L-1]\n    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n    \n    for l in reversed(range(L-1)):\n        # lth layer: (RELU -&gt; LINEAR) gradients.\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n\n    return grads\n\ndef update_parameters(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using gradient descent\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients, output of L_model_backward\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n                  parameters[\"W\" + str(l)] = ... \n                  parameters[\"b\" + str(l)] = ...\n    \"\"\"\n    \n    L = len(parameters) // 2 # number of layers in the neural network\n\n    # Update rule for each parameter. Use a for loop.\n    for l in range(L):\n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n        \n    return parameters"
  },
  {
    "objectID": "Cat_classifier.html#model-training",
    "href": "Cat_classifier.html#model-training",
    "title": "Building a Cat Image Classifier using Neural Networks from Scratch",
    "section": "Model Training",
    "text": "Model Training\n\nTraining of Deep Neural Network Models\nThe function L_layer_model implements the training process for both two-layer and L-layer neural network models. It employs a deep neural network architecture of the form: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID, allowing flexibility in defining the number of layers and their respective sizes.\nKey Components:\n\nInitialization: The function initializes the parameters of the neural network using a deep initialization method via initialize_parameters_deep.\nIterations (Gradient Descent): It performs a specified number of iterations (controlled by num_iterations) to optimize the parameters. During each iteration:\n\nForward Propagation: The forward pass computes the activations (AL) and caches through the neural network layers using L_model_forward.\nCost Computation: The cost function is computed to evaluate the performance of the network through compute_cost.\nBackward Propagation: The gradients are calculated using backpropagation via L_model_backward.\nParameter Update: The parameters are updated using the calculated gradients and a specified learning rate through update_parameters.\n\nCost Tracking: It maintains a record of the cost after every 100 iterations (or the final iteration if specified) and appends it to the costs list.\nPrint Cost (Optional): The parameter print_cost enables the display of the cost value at specified intervals (every 100 iterations) for tracking the convergence of the model during training.\n\nReturns:\nThe function returns the optimized parameters learned during training (parameters) and a list of costs (costs) tracked over the iterations, which can be useful for visualizing the learning curve and assessing convergence.\nUsage:\nThis function is versatile, allowing the training of deep neural networks by defining the number of layers (layers_dims), input data (X), true labels (Y), learning rate (learning_rate), and the number of optimization iterations (num_iterations). The print_cost flag controls whether the cost is printed during training iterations, providing flexibility for monitoring training progress.\n\ndef L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.\n    \n    Arguments:\n    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    np.random.seed(1)\n    costs = []  # keep track of cost\n    \n    # Parameters initialization\n    parameters = initialize_parameters_deep(layers_dims)\n    \n    # Loop (gradient descent)\n    for i in range(num_iterations):\n\n        # Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID\n        AL, caches = L_model_forward(X, parameters)\n        \n        # Compute cost\n        cost = compute_cost(AL, Y)\n    \n        # Backward propagation\n        grads = L_model_backward(AL, Y, caches)\n \n        # Update parameters\n        parameters = update_parameters(parameters, grads, learning_rate)\n                \n        # Print the cost every 100 iterations\n        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if i % 100 == 0 or i == num_iterations - 1:\n            costs.append(cost)\n    \n    return parameters, costs\n\n\n\nGradient Descent and Parameter Updates\nThis section involves the iterative process of optimizing the neural network through gradient descent and updating the model parameters. The L_layer_model function, called with specified parameters like num_iterations = 2500 and print_cost = True, encapsulates the core of this process.\n\nGradient Descent Iterations: The function initiates a loop for a specified number of iterations (num_iterations = 2500) where the model undergoes forward and backward propagation to compute the gradients and update the parameters.\nCost Evaluation: During each iteration, the cost (or loss) is computed and printed if print_cost is set to True. This allows monitoring the model’s learning progress over the iterations.\nParameter Updates: Through backpropagation, gradients are computed for each layer, and the parameters (weights and biases) are updated using a learning rate and the calculated gradients to minimize the cost function.\n\nThis section represents the heart of training the neural network, where the model learns from the training data by adjusting its parameters iteratively to minimize the cost, eventually improving its predictive capability.\n\nparameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)\n\nCost after iteration 0: 0.7717493284237686\nCost after iteration 100: 0.6720534400822914\nCost after iteration 200: 0.6482632048575212\nCost after iteration 300: 0.6115068816101354\nCost after iteration 400: 0.5670473268366111\nCost after iteration 500: 0.5401376634547801\nCost after iteration 600: 0.5279299569455267\nCost after iteration 700: 0.4654773771766851\nCost after iteration 800: 0.36912585249592794\nCost after iteration 900: 0.39174697434805344\nCost after iteration 1000: 0.3151869888600617\nCost after iteration 1100: 0.2726998441789385\nCost after iteration 1200: 0.23741853400268134\nCost after iteration 1300: 0.19960120532208644\nCost after iteration 1400: 0.18926300388463305\nCost after iteration 1500: 0.16118854665827748\nCost after iteration 1600: 0.14821389662363316\nCost after iteration 1700: 0.13777487812972944\nCost after iteration 1800: 0.12974017549190123\nCost after iteration 1900: 0.12122535068005211\nCost after iteration 2000: 0.11382060668633713\nCost after iteration 2100: 0.10783928526254132\nCost after iteration 2200: 0.10285466069352679\nCost after iteration 2300: 0.10089745445261787\nCost after iteration 2400: 0.09287821526472397\nCost after iteration 2499: 0.088439943441702\n\n\nObservations:\n\nCost Decrease: The cost reduces progressively as the number of iterations increases. This reduction indicates that the model is learning and optimizing its parameters to better fit the training data.\nConvergence: Initially, the cost starts relatively high at 0.7717 and consistently decreases with each iteration. As the iterations progress, the rate of reduction in the cost diminishes, implying that the model is converging towards an optimal solution.\nStabilization: After around 2000 iterations, the cost reduction slows down considerably, with smaller decreases in subsequent iterations. This signifies that the model’s improvement becomes marginal, indicating it is approaching convergence or an optimal solution.\n\nOverall, the output demonstrates the iterative nature of the training process, showing how the neural network learns and adjusts its parameters to minimize the cost function, thereby enhancing its predictive capability on the training data."
  },
  {
    "objectID": "Cat_classifier.html#model-evaluation",
    "href": "Cat_classifier.html#model-evaluation",
    "title": "Building a Cat Image Classifier using Neural Networks from Scratch",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nModel Performance Evaluation and Accuracy Assessment\nThis section evaluates the performance of the trained L-layer neural network model by assessing its prediction accuracy on both the training and test datasets. The function predict() is utilized to make predictions on the provided datasets (train_x and test_x) using the trained model parameters.\nThe function performs the following steps: - Takes the dataset examples (X) and corresponding labels (y) as inputs along with the trained model parameters. - Utilizes forward propagation through the neural network to obtain predictions (p) for the given dataset X. - Converts the raw probabilities (probas) into binary predictions (0 or 1) based on a threshold of 0.5. - Calculates and displays the accuracy of the predictions by comparing them with the true labels (y).\n\ndef predict(X, y, parameters):\n    \"\"\"\n    This function is used to predict the results of a  L-layer neural network.\n    \n    Arguments:\n    X -- data set of examples you would like to label\n    parameters -- parameters of the trained model\n    \n    Returns:\n    p -- predictions for the given dataset X\n    \"\"\"\n    \n    m = X.shape[1]\n    n = len(parameters) // 2 # number of layers in the neural network\n    p = np.zeros((1,m))\n    \n    # Forward propagation\n    probas, caches = L_model_forward(X, parameters)\n\n    \n    # convert probas to 0/1 predictions\n    for i in range(0, probas.shape[1]):\n        if probas[0,i] &gt; 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n    \n    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n        \n    return p\n\n\npred_train = predict(train_x, train_y, parameters)\n\nAccuracy: 0.9856459330143539\n\n\n\npred_test = predict(test_x, test_y, parameters)\n\nAccuracy: 0.8\n\n\nThe reported accuracy for the training dataset is approximately 98.56%, while the accuracy for the test dataset stands at 80.0%. Assessing the accuracy on both datasets provides insights into the model’s performance on seen and unseen data, indicating its capability to generalize beyond the training set."
  },
  {
    "objectID": "Cat_classifier.html#results-visualization",
    "href": "Cat_classifier.html#results-visualization",
    "title": "Building a Cat Image Classifier using Neural Networks from Scratch",
    "section": "Results Visualization",
    "text": "Results Visualization\n\nCost Evolution During Training Iterations\nThe visualization depicts the evolution of the cost or loss function over training iterations. It is generated using the plot_costs function, which takes in the costs array as input. The y-axis represents the cost value, while the x-axis shows the iterations in multiples of hundreds.\nThe plot demonstrates the decreasing trend of the cost function with increasing iterations, indicating the model’s learning progress. It’s evident from the displayed cost values after each iteration that the cost gradually decreases. Initially, the cost is relatively high, reflecting the model’s higher error rate, but it decreases consistently over training epochs. As the iterations progress, the cost approaches a lower value, signifying the model’s improvement in minimizing errors and getting closer to optimal parameters.\nThe plotted curve illustrates how the cost decreases over time, providing insight into the model’s learning behavior and convergence towards better performance.\n\ndef plot_costs(costs, learning_rate=0.0075):\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n\nplot_costs(costs, learning_rate)\n\n\n\n\n\n\nVisualizing Misclassified Images\nThis section utilizes a function named print_mislabeled_images() to visually represent misclassified images. The function takes parameters including the class labels, dataset (X), true labels (y), and predicted labels (p). It aims to plot and display images where the predictions made by the model differ from the true labels.\nThe code identifies and extracts misclassified indices, highlighting cases where the sum of predicted and true labels is equal to 1. It then generates visualizations for these misclassified images by iterating through the identified indices. The plot showcases the image, its predicted class label, and the actual class label.\nIn this specific instance, the function displays 10 misclassified images, enabling an insightful view into instances where the model’s predictions did not align with the ground truth labels. The visualization aids in understanding the nature of misclassifications and potentially identifying patterns or challenges within the model’s performance.\n\ndef print_mislabeled_images(classes, X, y, p):\n    \"\"\"\n    Plots images where predictions and truth were different.\n    X -- dataset\n    y -- true labels\n    p -- predictions\n    \"\"\"\n    a = p + y\n    mislabeled_indices = np.asarray(np.where(a == 1))\n    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n    num_images = len(mislabeled_indices[0])\n    for i in range(num_images):\n        index = mislabeled_indices[1][i]\n        \n        plt.subplot(2, num_images, i + 1)\n        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n        plt.axis('off')\n        plt.title(\"Prediction: \" + classes[int(p[0,index])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[0,index]].decode(\"utf-8\"))\n\nprint_mislabeled_images(classes, test_x, test_y, pred_test)"
  },
  {
    "objectID": "Cat_classifier.html#conclusion-assessing-the-cat-image-classification-neural-network",
    "href": "Cat_classifier.html#conclusion-assessing-the-cat-image-classification-neural-network",
    "title": "Building a Cat Image Classifier using Neural Networks from Scratch",
    "section": "Conclusion: Assessing the Cat Image Classification Neural Network",
    "text": "Conclusion: Assessing the Cat Image Classification Neural Network\nThe development of the cat image classification neural network showcases the application of deep learning techniques for image recognition tasks. Throughout this project, several key aspects have been addressed:\nModel Architecture and Training: The notebook implemented an L-layer neural network from scratch using numpy and scipy. The architecture included linear and activation functions (ReLU and sigmoid) and underwent iterative training to optimize parameters for better accuracy.\nPerformance Evaluation: The trained models were evaluated on both the training and test sets. The assessment included metrics like accuracy, cost, and visual representation of incorrectly classified images. These evaluations provided insights into the model’s ability to distinguish cat images from non-cat images.\nKey Findings: The models exhibited varying levels of performance. The L-layer network demonstrated improved accuracy and better representation of features.\nChallenges and Future Directions: Despite the successful implementation, challenges like overfitting, optimization convergence, and computational intensity were encountered. To further enhance the model’s performance, regularization techniques, hyperparameter tuning, and exploring more complex architectures could be considered.\nApplication and Relevance: The cat image classification neural network exemplifies the practical application of machine learning in real-world scenarios, specifically in image recognition tasks. The skills acquired and lessons learned from this project lay a solid foundation for tackling more complex image classification problems.\nFinal Thoughts: Building a neural network from scratch not only provided a deep understanding of its inner workings but also emphasized the significance of data preprocessing, model architecture, and iterative optimization in achieving robust and accurate predictions.\nIn essence, this project not only successfully classified cat images but also served as a stepping stone towards understanding and refining neural networks for diverse image recognition applications."
  }
]